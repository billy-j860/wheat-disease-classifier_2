{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZIgE0aoyAY0"
   },
   "source": [
    "### 1. Project Overview\n",
    "\n",
    "##### Problem statement\n",
    "    Smallholder wheat farmers need a tool to diagnose pests and diseases in wheat crops to prevent yield loss.\n",
    "##### Goal\n",
    "    Develop an image classification model for diagnosing wheat pests and diseases, which will be deployable on a web platform for smallholder farmers.\n",
    "##### Data Source\n",
    "    The dataset is sourced from https://www.kaggle.com/datasets/kushagra3204/wheat-plant-diseases\n",
    "##### Pests and Disease Classes\n",
    "- **Pests**: Aphid\n",
    "- **Diseases**: Yellow Rust, Mildew, Fusarium Head Blight\n",
    "- **Healthy**: A class representing healthy crops without pests or diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofN0ary9yAY2"
   },
   "source": [
    "## Wheat Plant Disease Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaCvWGYyyAY3"
   },
   "outputs": [],
   "source": [
    "#First, we'll import all the necessary libraries to aid in this task\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import logging\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZDI_Nva7TIk"
   },
   "source": [
    "### Initial Setup and Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNwUWn_8yAY5"
   },
   "outputs": [],
   "source": [
    "# Then, we'll suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmuiGOyDyAYu",
    "outputId": "d264bb8b-6366-4abb-f07a-1a13223944bc"
   },
   "outputs": [],
   "source": [
    "#Then we import the required Kaggle library to download our dataset.\n",
    "import kagglehub\n",
    "kushagra3204_wheat_plant_diseases_path = kagglehub.dataset_download('kushagra3204/wheat-plant-diseases')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlSyDM038oaB"
   },
   "source": [
    "### Directory Structure Setup\n",
    "Now we'll set up our directory paths to access the train, test, and validation data. This dataset follows the standard machine learning directory structure with three main splits:\n",
    "- Training data: Used to train the model\n",
    "- Test data: Used for final model evaluation\n",
    "- Validation data: Used during training to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvEUpB1U4vzV"
   },
   "outputs": [],
   "source": [
    "# Set up base path and separate paths for train, test, and validation data\n",
    "base_path = kushagra3204_wheat_plant_diseases_path\n",
    "train_path = os.path.join(base_path, 'data', 'train')\n",
    "test_path = os.path.join(base_path, 'data', 'test')\n",
    "valid_path = os.path.join(base_path, 'data', 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYlTt8Mh46BD",
    "outputId": "d550deb3-e703-441d-f48e-0a079f984153"
   },
   "outputs": [],
   "source": [
    "# Verify the contents of each directory\n",
    "print(\"Training data contents:\")\n",
    "print(os.listdir(train_path))\n",
    "\n",
    "print(\"\\nTest data contents:\")\n",
    "print(os.listdir(test_path))\n",
    "\n",
    "print(\"\\nValidation data contents:\")\n",
    "print(os.listdir(valid_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaLW7zeVyAY5"
   },
   "source": [
    "### Class Selection for Wheat Plant Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsCxJ2MT-0vK"
   },
   "source": [
    "### Selected Classes\n",
    "We'll focus on the following classes as per requirements:\n",
    "- **Pest (1)**: Aphid\n",
    "- **Diseases (3)**:\n",
    "  - Yellow Rust\n",
    "  - Fusarium Head Blight\n",
    "  - Mildew\n",
    "- **Control**: Healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vbF-ogY-_PC"
   },
   "source": [
    "#### Class Mapping Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RNG_epm-7Hm"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map classes across train, validation, and test sets\n",
    "class_mappings = {\n",
    "    'train': ['Aphid', 'Yellow Rust', 'Fusarium Head Blight', 'Mildew', 'Healthy'],\n",
    "    'valid': ['aphid_valid', 'yellow_rust_valid', 'fusarium_head_blight_valid', 'mildew_valid', 'healthy_valid'],\n",
    "    'test': ['aphid_test', 'yellow_rust_test', 'fusarium_head_blight_test', 'mildew_test', 'healthy_test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_TNhtSl_UFD",
    "outputId": "86f45497-02d6-47e0-eb14-02d6dba66914"
   },
   "outputs": [],
   "source": [
    "#class verification\n",
    "print(\"Checking if all required classes exist:\")\n",
    "\n",
    "# Training set check\n",
    "train_classes = os.listdir(train_path)\n",
    "print(\"\\nTraining classes found:\", [cls for cls in class_mappings['train'] if cls in train_classes])\n",
    "\n",
    "# Test set check\n",
    "test_classes = os.listdir(test_path)\n",
    "print(\"\\nTest classes found:\", [cls for cls in class_mappings['test'] if cls in test_classes])\n",
    "\n",
    "# Validation set check\n",
    "valid_classes = os.listdir(valid_path)\n",
    "print(\"\\nValidation classes found:\", [cls for cls in class_mappings['valid'] if cls in valid_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GSA-KWwCvBE"
   },
   "source": [
    "## Exploratory Data Analysis (EDA) of Wheat Disease Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl5XUrXBLdjQ"
   },
   "source": [
    "### **1.** Display Sample Images from Each Class and Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bSISriIQLjmZ",
    "outputId": "b31f5f6f-7ad4-4cc3-fd02-1bb75c454de0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Sample Images: Training, Test, and Validation Sets', fontsize=16)\n",
    "\n",
    "# Counter for subplot positions\n",
    "plot_idx = 1\n",
    "\n",
    "# Loop through each class\n",
    "for class_idx in range(5):\n",
    "    # Get class names for each set\n",
    "    train_class = class_mappings['train'][class_idx]\n",
    "    test_class = class_mappings['test'][class_idx]\n",
    "    valid_class = class_mappings['valid'][class_idx]\n",
    "\n",
    "    # Training set sample\n",
    "    plt.subplot(5, 3, plot_idx)\n",
    "    img_path = os.path.join(train_path, train_class, random.choice(os.listdir(os.path.join(train_path, train_class))))\n",
    "    img = np.array(Image.open(img_path))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Train: {train_class}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Test set sample\n",
    "    plt.subplot(5, 3, plot_idx + 1)\n",
    "    img_path = os.path.join(test_path, test_class, random.choice(os.listdir(os.path.join(test_path, test_class))))\n",
    "    img = np.array(Image.open(img_path))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Test: {test_class}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Validation set sample\n",
    "    plt.subplot(5, 3, plot_idx + 2)\n",
    "    img_path = os.path.join(valid_path, valid_class, random.choice(os.listdir(os.path.join(valid_path, valid_class))))\n",
    "    img = np.array(Image.open(img_path))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Valid: {valid_class}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Move to next row\n",
    "    plot_idx += 3\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEkzA0j0C8M9"
   },
   "source": [
    "### **2**. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEiKASCbAP-p",
    "outputId": "73e6475c-b941-4d90-9ade-c22edf40ffc5"
   },
   "outputs": [],
   "source": [
    "# Analyze distribution across train, test, and validation sets\n",
    "print(\"\\nClass Distribution across classes\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def analyze_split(split_name, split_path, classes):\n",
    "    #Analyze image distribution in a dataset split\"\"\"\n",
    "    print(f\"\\n{split_name} Set Distribution:\")\n",
    "    total_images = 0\n",
    "    for class_name in classes:\n",
    "        num_images = len(os.listdir(os.path.join(split_path, class_name)))\n",
    "        total_images += num_images\n",
    "        print(f\"{class_name}: {num_images} images\")\n",
    "    print(f\"Total images in {split_name.lower()} set: {total_images}\")\n",
    "    return total_images\n",
    "# Analyze each split\n",
    "train_total = analyze_split(\"Training\", train_path, class_mappings['train'])\n",
    "test_total = analyze_split(\"Test\", test_path, class_mappings['test'])\n",
    "valid_total = analyze_split(\"Validation\", valid_path, class_mappings['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVtk-jexGZm9"
   },
   "source": [
    "### **3**. Visualization of Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "-zulazf5Go46",
    "outputId": "310b693a-d95b-40ce-e827-8e5d85d5f025"
   },
   "outputs": [],
   "source": [
    "# Create bar plot of training set distribution this helps to check class imbalance\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts = [len(os.listdir(os.path.join(train_path, cls))) for cls in class_mappings['train']]\n",
    "plt.bar(class_mappings['train'], class_counts)\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg-qEOZKG5Kl"
   },
   "source": [
    "### **4.** Image Properties Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZetAnbHHLJG"
   },
   "outputs": [],
   "source": [
    "# Function to get unique image properties (5 samples)\n",
    "def get_unique_properties(dataset_path, classes):\n",
    "    unique_props = set()\n",
    "\n",
    "    # Loop through each class\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "\n",
    "        # Get images in the class folder\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img = Image.open(os.path.join(class_path, img_name))\n",
    "            # Create tuple of properties\n",
    "            props = (class_name, img.size, img.format, img.mode)\n",
    "            unique_props.add(props)\n",
    "\n",
    "            # Break if we have 5 unique combinations\n",
    "            if len(unique_props) >= 5:\n",
    "                break\n",
    "        if len(unique_props) >= 5:\n",
    "            break\n",
    "\n",
    "    return list(unique_props)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "et19JP9nIjLQ",
    "outputId": "22153383-c163-4194-967d-6a574e0e52f7"
   },
   "outputs": [],
   "source": [
    "# Analyze and display results for each set\n",
    "print(\"\\nTraining Set:\")\n",
    "train_props = get_unique_properties(train_path, class_mappings['train'])\n",
    "for class_name, size, format, mode in train_props:\n",
    "    print(f\"{class_name}: Size {size}, Format {format}, Mode {mode}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "test_props = get_unique_properties(test_path, class_mappings['test'])\n",
    "for class_name, size, format, mode in test_props:\n",
    "    print(f\"{class_name}: Size {size}, Format {format}, Mode {mode}\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "valid_props = get_unique_properties(valid_path, class_mappings['valid'])\n",
    "for class_name, size, format, mode in valid_props:\n",
    "    print(f\"{class_name}: Size {size}, Format {format}, Mode {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkNJL42I9OQB"
   },
   "outputs": [],
   "source": [
    "def analyze_dataset(base_path, class_mappings):\n",
    "    print(\"Initial Data Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for class_name in class_mappings['train']:\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        images = os.listdir(class_path)\n",
    "\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        print(f\"Total images: {len(images)}\")\n",
    "\n",
    "        # Check image properties\n",
    "        formats = set()\n",
    "        modes = set()\n",
    "        sizes = set()\n",
    "        corrupted = 0\n",
    "\n",
    "        for img_name in images:\n",
    "            try:\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                with Image.open(img_path) as img:\n",
    "                    formats.add(img.format)\n",
    "                    modes.add(img.mode)\n",
    "                    sizes.add(img.size)\n",
    "            except:\n",
    "                corrupted += 1\n",
    "\n",
    "        print(f\"Formats found: {formats}\")\n",
    "        print(f\"Color modes: {modes}\")\n",
    "        print(f\"Image sizes: {sizes}\")\n",
    "        print(f\"Corrupted images: {corrupted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubF-AlxC9ON4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4X2hmrYyAZA"
   },
   "source": [
    "## Data Cleaning & Initial Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpirkfGHf1Fq"
   },
   "source": [
    "##### Training Data Analysis observation\n",
    "1. **Class** **imbalance** **in** **training** **set** **:** Yellow Rust (1301): most represented\n",
    "Fusarium Head Blight (611):severely underrepresented\n",
    "Others (Mildew, Healthy, Aphid): relatively balanced (~900-1000)\n",
    "2. **Image** **format** **inconsistencies** **:** Mix of JPEG and PNG formats, Multiple image sizes observed, Most images are RGB, others are RGBA rtc\n",
    "3.  **Quality** **Concerns** **:** Text overlays on some image, Varying backgrounds, Different angles and zoom levels, Some images appear darker/lighter than others\n",
    "##### Data cleaning and preprocessing pipeline includes:\n",
    "- Image standardization (size: 224x224)\n",
    "- Format conversion to RGB\n",
    "- Corruption removal\n",
    "- Class balancing through augmentation\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvcHRLNWSfjI"
   },
   "source": [
    "#### **1**. Handling corrupted Images(Train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pUwxdQiSfEz"
   },
   "outputs": [],
   "source": [
    "# Check and remove images that can't be opened or are corrupted\n",
    "def remove_corrupted_images(class_path):\n",
    "    corrupted = []\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()\n",
    "                #load as array to catch more issues\n",
    "                img_array = np.array(img)\n",
    "        except:\n",
    "            corrupted.append(img_path)\n",
    "            os.remove(img_path)\n",
    "    return corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3RJaes9paR9"
   },
   "source": [
    "##### **2**. Finding and removing Duplicates(Train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-rR8gcypg-j"
   },
   "outputs": [],
   "source": [
    "#Remove duplicate images using hash comparison\n",
    "def remove_duplicate_images(class_path):\n",
    "    hash_dict = {}\n",
    "    duplicates = []\n",
    "\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                # Calculate image hash\n",
    "                img_hash = hashlib.md5(np.array(img).tobytes()).hexdigest()\n",
    "\n",
    "                if img_hash in hash_dict:\n",
    "                    duplicates.append(img_path)\n",
    "                    os.remove(img_path)\n",
    "                else:\n",
    "                    hash_dict[img_hash] = img_path\n",
    "        except:\n",
    "            continue\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-WZ0LO9Vf4l"
   },
   "source": [
    "##### **3**. Image quality Standardization(train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyLiNfoLVOtd"
   },
   "outputs": [],
   "source": [
    "#Standardize image format and size\n",
    "def standardize_image(img, target_size=(224, 224)):\n",
    "    # Convert to RGB\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    # Resize\n",
    "    img = img.resize(target_size)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5Qxt09rWJ4Y"
   },
   "source": [
    "#### **4**. Cleaning pipeline (Train dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bv4578WBWIDv",
    "outputId": "3b11c228-4bb5-4957-930f-3739682965e2"
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    This function is pipeline to execute cleaned and preprocessed training images:\n",
    "    1. Remove corrupted images\n",
    "    2. Convert to standard format\n",
    "    3.Resize to standard size\n",
    "    \"\"\"\n",
    "def clean_training_data():\n",
    "  for class_name in class_mappings['train']:\n",
    "        print(f\"\\nProcessing {class_name}\")\n",
    "        class_path = os.path.join(train_path, class_name)\n",
    "        removed = 0\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "            try:\n",
    "                # Open and process image\n",
    "                img = Image.open(img_path)\n",
    "\n",
    "                # Convert to RGB\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "\n",
    "                # Resize to standard size\n",
    "                img = img.resize((224, 224))\n",
    "\n",
    "                # Save processed image\n",
    "                img.save(img_path, 'JPEG')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Removing corrupted image: {img_name}\")\n",
    "                os.remove(img_path)\n",
    "                removed += 1\n",
    "\n",
    "        print(f\"Removed {removed} corrupted images\")\n",
    "        print(f\"Final count: {len(os.listdir(class_path))} images\")\n",
    "# Run cleaning\n",
    "clean_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWzPR4kvxqp5",
    "outputId": "ae1d25dc-fbfe-4ff3-e108-f197cf37f5ae"
   },
   "outputs": [],
   "source": [
    "#verify our cleaned images\n",
    "for class_name in class_mappings['train']:\n",
    "    class_path = os.path.join(train_path, class_name)\n",
    "    count = len(os.listdir(class_path))\n",
    "    print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwJKEl0oWYA6"
   },
   "source": [
    "#### 5. Balancing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOo9kysCvE0v"
   },
   "source": [
    "###### a. Balance dataset through augmentation\n",
    "        Augment each class to reach atleast 1000 images.\n",
    "        train_path: Path to training data\n",
    "        class_mappings: Dictionary of class names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmM-MyNQxGao",
    "outputId": "a79ea7d1-c897-4ef8-b5ea-e2aa3c60f71d"
   },
   "outputs": [],
   "source": [
    "# Based on our cleaned data the distribution is highly imbalanced:\n",
    "# Yellow Rust: 1301 (Majority class)\n",
    "# Mildew: 1081\n",
    "# Healthy: 1000\n",
    "# Aphid: 903\n",
    "# Fusarium Head Blight: 611 (Minority class)\n",
    "# Data Balancing through Augmentation, recommendation of minimum 1000 images per class\n",
    "\n",
    "def balance_dataset(train_path, class_mappings, target_count=1000):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    for class_name in class_mappings['train']:\n",
    "        class_path = os.path.join(train_path, class_name)\n",
    "        current_count = len(os.listdir(class_path))\n",
    "\n",
    "        if current_count < target_count:\n",
    "            needed = target_count - current_count\n",
    "            print(f\"Augmenting {class_name} with {needed} images\")\n",
    "\n",
    "            files = os.listdir(class_path)\n",
    "            for i in range(needed):\n",
    "                img_path = os.path.join(class_path, random.choice(files))\n",
    "                img = np.array(Image.open(img_path))\n",
    "                img = img.reshape((1,) + img.shape)\n",
    "\n",
    "                aug_img = next(datagen.flow(img, batch_size=1))[0].astype(np.uint8)\n",
    "                Image.fromarray(aug_img).save(\n",
    "                    os.path.join(class_path, f'aug_{i}.jpg'),\n",
    "                    'JPEG',\n",
    "                    quality=95\n",
    "                )\n",
    "\n",
    "# Balance the dataset\n",
    "balance_dataset(train_path, class_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. cleaning and preprocessing validation set\n",
    "- Similar cleaning and standardization as trainset\n",
    "- No augmentation to maintain evaluation integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_validation_data():\n",
    "    \n",
    "    for class_name in class_mappings['valid']:\n",
    "        print(f\"\\nProcessing {class_name}\")\n",
    "        class_path = os.path.join(valid_path, class_name)\n",
    "        removed = 0\n",
    "        \n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                # Open and process image\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                # Convert to RGB\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                \n",
    "                # Resize to standard size\n",
    "                img = img.resize((224, 224))\n",
    "                \n",
    "                # Save processed image\n",
    "                img.save(img_path, 'JPEG')\n",
    "                \n",
    "            except:\n",
    "                print(f\"Removing corrupted image: {img_name}\")\n",
    "                os.remove(img_path)\n",
    "                removed += 1\n",
    "        \n",
    "        print(f\"Removed {removed} corrupted images\")\n",
    "        print(f\"Final count: {len(os.listdir(class_path))} images\")\n",
    "clean_validation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7**. Clean and preprocess test data\n",
    "- Similar cleaning and standardization\n",
    "- No augmentation to maintain evaluation integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_test_data():\n",
    "    print(\"\\nProcessing test data\")\n",
    "    \n",
    "    for class_name in class_mappings['test']:\n",
    "        print(f\"\\nProcessing {class_name}\")\n",
    "        class_path = os.path.join(test_path, class_name)\n",
    "        removed = 0\n",
    "        \n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                # Open and process image\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                # Convert to RGB\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                \n",
    "                # Resize to standard size\n",
    "                img = img.resize((224, 224))\n",
    "                \n",
    "                # Save processed image\n",
    "                img.save(img_path, 'JPEG')\n",
    "                \n",
    "            except:\n",
    "                print(f\"Removing corrupted image: {img_name}\")\n",
    "                os.remove(img_path)\n",
    "                removed += 1\n",
    "        \n",
    "        print(f\"Removed {removed} corrupted images\")\n",
    "        print(f\"Final count: {len(os.listdir(class_path))} images\")\n",
    "clean_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRpWNdOO07nm"
   },
   "source": [
    "##  Inspection After Cleaning, Balancing and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loALxYnh0_-o"
   },
   "source": [
    "### 1. Image count and class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SExuJbbDdju5",
    "outputId": "7aa5d027-aad0-4996-f0d2-81eb448bb3f5"
   },
   "outputs": [],
   "source": [
    "print(\"Class train distribution\")\n",
    "for class_name in class_mappings['train']:\n",
    "    total = len(os.listdir(os.path.join(train_path, class_name)))\n",
    "    print(f\"{class_name}: {total} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkkoVZ3k19Hu"
   },
   "source": [
    "### 2.  Final train image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pwax9cp2Iz0",
    "outputId": "fe313e0a-9451-4482-dbfe-cec1256339a5"
   },
   "outputs": [],
   "source": [
    "def view_final_properties():\n",
    "    print(\"\\nImage properties\")\n",
    "\n",
    "    for class_name in class_mappings['train']:\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        class_path = os.path.join(train_path, class_name)\n",
    "\n",
    "        # Get random sample image\n",
    "        sample_img_path = os.path.join(class_path, os.listdir(class_path)[0])\n",
    "        img = Image.open(sample_img_path)\n",
    "\n",
    "        print(f\"Size: {img.size}\")\n",
    "        print(f\"Mode: {img.mode}\")\n",
    "        print(f\"Format: {img.format}\")\n",
    "\n",
    "# View properties\n",
    "view_final_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6qMKKZJS2jtQ",
    "outputId": "4d505894-d386-4a27-8086-e655689760df"
   },
   "outputs": [],
   "source": [
    "# Display random images per class\n",
    "def view_sample_images():\n",
    "   plt.figure(figsize=(15, 10))\n",
    "\n",
    "   for idx, class_name in enumerate(class_mappings['train']):\n",
    "       class_path = os.path.join(train_path, class_name)\n",
    "       images = os.listdir(class_path)\n",
    "\n",
    "       # Select two random images\n",
    "       sample_images = random.sample(images, 2)\n",
    "\n",
    "       # Display first image\n",
    "       plt.subplot(5, 2, 2*idx + 1)\n",
    "       img1 = Image.open(os.path.join(class_path, sample_images[0]))\n",
    "       plt.imshow(img1)\n",
    "       plt.title(f\"{class_name} - Image 1\")\n",
    "       plt.axis('off')\n",
    "\n",
    "       # Display second image\n",
    "       plt.subplot(5, 2, 2*idx + 2)\n",
    "       img2 = Image.open(os.path.join(class_path, sample_images[1]))\n",
    "       plt.imshow(img2)\n",
    "       plt.title(f\"{class_name} - Image 2\")\n",
    "       plt.axis('off')\n",
    "\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "# View sample images\n",
    "view_sample_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample images after processing (validation and test datasets)\n",
    "def view_processed_images():\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Show validation samples\n",
    "    for idx, class_name in enumerate(class_mappings['valid']):\n",
    "        class_path = os.path.join(valid_path, class_name)\n",
    "        img_name = random.choice(os.listdir(class_path))\n",
    "        img = Image.open(os.path.join(class_path, img_name))\n",
    "        \n",
    "        plt.subplot(5, 2, 2*idx + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Validation: {class_name}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Show test samples\n",
    "        test_class = class_mappings['test'][idx]\n",
    "        class_path = os.path.join(test_path, test_class)\n",
    "        img_name = random.choice(os.listdir(class_path))\n",
    "        img = Image.open(os.path.join(class_path, img_name))\n",
    "        \n",
    "        plt.subplot(5, 2, 2*idx + 2)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Test: {test_class}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "view_processed_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARtIUIIsyAZP"
   },
   "source": [
    "## Model Development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20CA37aNO_un"
   },
   "source": [
    "### Set up Data generators\n",
    "###### Three separate generators for:\n",
    "- Training (with augmentation)\n",
    "- Validation (no augmentation)\n",
    "- Test (no augmentation)\n",
    "\n",
    "Image size: 160x160\n",
    "Batch size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIVQaLK8PLvn",
    "outputId": "6aba8bbb-c48b-41d6-b133-0bd0a24da8f7"
   },
   "outputs": [],
   "source": [
    "#First, I'm setting up my data generators with augmentation to help work with overfitting.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8,1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Using simple preprocessing for validation and test\n",
    "valid_test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    classes=class_mappings['train'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = valid_test_datagen.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    classes=class_mappings['valid'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = valid_test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    classes=class_mappings['test'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create callbacks for model training:\n",
    " #    - Early stopping to prevent overfitting\n",
    " #    - Model checkpoint to save best model\n",
    " #    - Learning rate reduction when model plateaus \n",
    "def create_callbacks(model_name):\n",
    "    callbacks = [\n",
    "        # Stop training when validation loss stops improving\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate when model plateaus\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        \n",
    "        # Save best model during training\n",
    "        ModelCheckpoint(\n",
    "            f'{model_name}_best_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeVtG2rkJy_0"
   },
   "source": [
    "### Model Architecture\n",
    "We'll implement and compare 3 different architectures:\n",
    "\n",
    "1. EfficientNetB0 - Efficient and accurate, good for moderate dataset sizes\n",
    "2. MobileNetV2 - Lighter and faster, good for mobile deployment\n",
    "3. ResNet50V2 - Deep and robust, good for complex feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tqq2hGHKLao"
   },
   "source": [
    "1. EfficientNetBO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hbnt-iFa6xgq"
   },
   "outputs": [],
   "source": [
    "# EfficientNetB0 Architecture\n",
    "#     - Pre-trained on ImageNet\n",
    "#     - Limited number of trainable layers\n",
    "#     - Custom classification head\n",
    "def create_efficient_model():\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze initial layers\n",
    "    for layer in base_model.layers[:-10]:  \n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSXxJ9VwKwQh"
   },
   "source": [
    "2. MobileNetV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyyeBHBzI7A8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BdcNd2hJ6xZD",
    "outputId": "9b692478-bf38-4ea4-f7be-30594e549e49"
   },
   "outputs": [],
   "source": [
    "# MobileNetV2 Architecture\n",
    "    # - Pre-trained on ImageNet\n",
    "    # - Optimized for mobile deployment\n",
    "    # - Custom classification head\n",
    "\n",
    "def create_mobilenet_model():\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze initial layers\n",
    "    for layer in base_model.layers[:-15]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUHSYVskLRjw"
   },
   "source": [
    "3. ResNet50V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB15uNjBLuAv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import  ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "_uge5dVH6xU3",
    "outputId": "63e9be5c-eeb1-49c9-cc69-ec56b1210284"
   },
   "outputs": [],
   "source": [
    "# ResNet50V2 Architecture\n",
    "    # - Pre-trained on ImageNet\n",
    "    # - Deep architecture with residual connections\n",
    "    # - Custom classification head\n",
    "\n",
    "def create_resnet_model():\n",
    "    base_model = ResNet50V2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze initial layers\n",
    "    for layer in base_model.layers[:-15]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWbvZJj-MPjl"
   },
   "source": [
    "### Training and Configuration of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles and trains the model\n",
    "def compile_and_train_model(model, model_name, train_generator, valid_generator):\n",
    "    # Use Adam optimizer\n",
    "    optimizer = Adam(\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-6\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'best_{model_name}.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=valid_generator,\n",
    "        epochs=30,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1  # Show progress bar\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKMP7pMlNL48"
   },
   "source": [
    "   Training EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yJBDOeq6xHs",
    "outputId": "e084715f-e888-47c6-bf2d-3daa81cf899c"
   },
   "outputs": [],
   "source": [
    "# Create and train EfficientNetB0 model\n",
    "efficient_model = create_efficient_model()\n",
    "efficient_history = compile_and_train_model(\n",
    "    efficient_model,\n",
    "    'efficientnet',\n",
    "    train_generator,\n",
    "    valid_generator\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EfficientNetB0 results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(efficient_history.history['accuracy'], label='Training')\n",
    "plt.plot(efficient_history.history['val_accuracy'], label='Validation')\n",
    "plt.title('EfficientNetB0 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(efficient_history.history['loss'], label='Training')\n",
    "plt.plot(efficient_history.history['val_loss'], label='Validation')\n",
    "plt.title('EfficientNetB0 Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mHCikc1JlpF"
   },
   "source": [
    "Training MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaTLNqAIZaOM",
    "outputId": "b2dde0df-faf7-4099-d790-0da723ee170e"
   },
   "outputs": [],
   "source": [
    "mobile_model = create_mobilenet_model()\n",
    "mobile_history = compile_and_train_model(\n",
    "    mobile_model,\n",
    "    'mobilenet',\n",
    "    train_generator,\n",
    "    valid_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MobileNetV2 results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mobile_history.history['accuracy'], label='Training')\n",
    "plt.plot(mobile_history.history['val_accuracy'], label='Validation')\n",
    "plt.title('MobileNetV2 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mobile_history.history['loss'], label='Training')\n",
    "plt.plot(mobile_history.history['val_loss'], label='Validation')\n",
    "plt.title('MobileNetV2 Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb4mYg5DR3G7"
   },
   "source": [
    "ResNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bydAlRkR8DR",
    "outputId": "58c90b15-a3f8-46f2-8659-a8bf0525e811"
   },
   "outputs": [],
   "source": [
    "# Create and train ResNet model with early stopping and learning rate reduction\n",
    "resnet_model = create_resnet_model()\n",
    "resnet_history = compile_and_train_model(\n",
    "    resnet_model,\n",
    "    'resnet',\n",
    "    train_generator,\n",
    "    valid_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ResNet50V2 results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(resnet_history.history['accuracy'], label='Training')\n",
    "plt.plot(resnet_history.history['val_accuracy'], label='Validation')\n",
    "plt.title('ResNet50V2 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(resnet_history.history['loss'], label='Training')\n",
    "plt.plot(resnet_history.history['val_loss'], label='Validation')\n",
    "plt.title('ResNet50V2 Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkAfKcxvR8Ak"
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "#efficient_model.save('models/efficientnet_best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "#test_loss, test_accuracy = efficient_model.evaluate(test_generator, verbose=1)\n",
    "#print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4611404,
     "sourceId": 8094053,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
